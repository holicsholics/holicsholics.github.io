---
layout: single
title:  'TIL_11월3일, Decision Tree Classfier"
---

##결정 트리 학습법
결정 트리 학습법(decision tree learning)은 예측 모델링 방법 중 하나로 어떤 항목에 대한 관측값과 목표값을 연결시켜주는 예측모델로서 결정 트리를 사용한다.
먼저 결정 트리의 특징에 대해서 알아보자.
결정트리
* 분할과 가지치기 과정을 반복하면서 모델을 생성한다.
* 결정트리에는 분류와 회귀 모두에 사용할 수 있다.
* 여러개의 모델을 함께 사용하는 앙상블 모델이 존재한다. (RandomForest, GradientBoosting, XGBoost)
* 각 특성이 개별 처리되기 때문에 데이터 스케일에 영향을 받지 않아 특성의 정규화나 표준화가 필요 없다.
* 시계열 데이터와 같이 범위 밖의 포인트는 예측 할 수 없다.
* 과대적합되는 경향이 있다. 이는 본문에 소개할 가지치기 기법을 사용해도 크게 개선되지 않는다.
결정트리에는 분류트리와 회귀트리가 있다.
분류트리 : 트리 모델 중 목표변수가 유한한 수의 값을 가짐
회귀트리 : 결정 트리 중 목표변수가 연속하는 값, 일반적으로 실수를 가짐
오늘 분류트리에 알아볼 것이다.
분류트리, DecisionTreeClassifier는
1. 여러가지 독립 변수 중 하나의 독립 변수를 선택하고 그 독립 변수에 대한 기준값(threshold)을 정한다. 이를 분류 규칙이라고 한다. 최적의 분류 규칙을 찾는 방법은 이후에 자세히 설명한다.
2. 전체 학습 데이터 집합(부모 노드)을 해당 독립 변수의 값이 기준값보다 작은 데이터 그룹(자식 노드 1)과 해당 독립 변수의 값이 기준값보다 큰 데이터 그룹(자식 노드 2)으로 나눈다.
3. 각각의 자식 노드에 대해 1~2의 단계를 반복하여 하위의 자식 노드를 만든다. 단, 자식 노드에 한가지 클래스의 데이터만 존재한다면 더 이상 자식 노드를 나누지 않고 중지한다.
이렇게 자식 노드 나누기를 연속적으로 적용하면 노드가 계속 증가하는 나무(tree)와 같은 형태로 표현할 수 있다.
분류규칙을 정하는 방법
분류 규칙을 정하는 방법은 부모 노드와 자식 노드 간의 엔트로피를 가장 낮게 만드는 최상의 독립 변수와 기준값을 찾는 것이다. 이러한 기준을 정량화한 것이 정보획득량(information gain)이다. 기본적으로 모든 독립 변수와 모든 가능한 기준값에 대해 정보획득량을 구하여 가장 정보획득량이 큰 독립 변수와 기준값을 선택한다.
Decision Tree의 구조
* 루트노드(Root Node) : 시작점
* 리프노드(Leaf Node) : 결정된 클래스 값
* 규칙노드/내부노드(Decision Node / Internal Node) : 데이터세트의 피처가 결합해 만들어진 분류를 위한 규칙조건

<img width="524" alt="스크린샷 2022-11-03 오전 10 31 52" src="https://user-images.githubusercontent.com/99530946/199671908-ffa17a29-6892-4dec-b74d-2d0b82c7a91d.png">

  
  
이 사진에서 각각의 노드를 알아보면
루트노드(Root Node) : 남자인가?
리프노드(Leaf Node) : 사망, 생존
규칙노드/내부노드(Decision Node / Internal Node) : (나이 > 9.5)인가?, (sibsp > 2.5)인가?
규칙이 많아질수록 분류 방식이 복잡해집니다. 이는 좋을 수도 있지만 자칫 과적합으로 이어질 가능성이 높다.
트리의 깊이(depth)가 깊어질수록 결정트리는 과적합되기 쉬워 예측 성능이 저하될 수 있다.
그러므로 우리는 가능한 적은 규칙노드로 높은 성능을 가지기 위해 데이터를 분류할 때 최대한 많은 데이터 셋이 해당 분류에 속할 수 있도록 규칙 노드의 규칙을 잘 정해야 한다.
2. Decision Tree의 장단점
장점
* 쉽고 직관적입니다.
* 각 피처의 스케일링과 정규화 같은 전처리 작업의 영향도가 크지 않습니다.
단점
* 규칙을 추가하며 서브트리를 만들어 나갈수록 모델이 복잡해지고, 과적합에 빠지기 쉽습니다.→ 트리의 크기를 사전에 제한하는 튜닝이 필요합니다.
3. Decision Tree Classifier의 파라미터 튜닝
파라미터 튜닝을 통해 트리를 잘 조합해서 과대적합 혹은 과소적합이 나오지 않게 하는 것이 중요함!

<img width="622" alt="스크린샷 2022-11-03 오전 10 32 04" src="https://user-images.githubusercontent.com/99530946/199671944-da3c235c-3c08-441e-bfcc-126c863ff096.png">


