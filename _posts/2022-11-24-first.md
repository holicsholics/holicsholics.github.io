---
layout: single
title:  "TIL_11월24일"
---

## 혼동행렬(Confusion Matrix)


11월 24일 목요일
(으쌰3_이거4조 저거4조 노션 정리본)

혼동행렬(Confusion Matrix)
* 모델의 성능을 평가할 때 사용되는 지표
* 예측값이 실제 관측값을 얼마나 정확히 예측했는지 보여주는 행렬
![Positive Negative](https://user-images.githubusercontent.com/99530946/203735439-2d5757e9-4bd1-4357-8e9c-00480cee89ad.png)

1. TP(True Positive) - 맞는 것을 올바르게 맞다고 예측한 것
![Positive Negative](https://user-images.githubusercontent.com/99530946/203735385-ea4cce0c-92eb-48f5-91ca-645b860503f7.png)

질병이 있는 사람(맞는 것)을 ‘올바르게(True) 맞다고(Positive) 예측’하였으니 이 경우는 TP
2. TN(True Negative) - 아닌 것을 올바르게 틀리다고 예측한 것
![disease absent!](https://user-images.githubusercontent.com/99530946/203735449-5f551d7e-f5ce-4826-82ac-60e06c3e9626.png)

질병이 없는 사람(아닌 것)을 ‘올바르게(True) 틀리다고(Negative) 예측’하였으니 TN 입니다.
3. FP(False Positive) - 아닌 것을 올바르지 않게 맞다고 예측한 것
![Positive Negative](https://user-images.githubusercontent.com/99530946/203735475-eb87c8ff-7800-4075-a073-8116b5d0638a.png)


질병이 없는 사람(아닌 것)을 ‘올바르지 않게(False) 맞다고(Positive) 예측’하였으니 FP입니다.
4. FN(False Negative) - 맞는 것을 올바르지 않게 틀리다고 예측한 것
![Positive Negative](https://user-images.githubusercontent.com/99530946/203735504-810048ec-d61b-4491-9444-97c6cc388f66.png)


질병이 있는 사람(맞는 것)을 ‘올바르지 않게(False) 틀리다고(Negative) 예측’하였으니 FN입니다.

사이킷런 metrics
```python
confusion_matrix(y_true, y_pred) 
# 실제값(y_true)과 예측값(y_pred)을 넣어 혼동행렬 만들기

# ex)
confusion_matrix(comp_df['실제값'],comp_df['예측값'])
```

##💡분류성능 평가지표
```python
accuracy_score(y_true, y_pred) # 정확도
precision_score(y_true, y_pred) # 정밀도
recall_score(y_true, y_pred) # 재현율
f1_score(y_true, y_pred) # f1 score
```

##🐾한걸음 더 나아가기
```python
fbeta_score(y_true, y_pred, beta) # F-beta score
classification_report(y_true, y_pred) # 분류성능 평가지표 
```
전체 제공
* F-beta score $β$ 매개변수를 사용하여 문제에 따라 정밀도와 재현율 간 균형에 가중치를 부여하는 방법을 결정하는 것. $β$값이 1일 경우 F1 score 와 동일한 결과가 나온다
* classification_report 분류성능평가지표 전체 결과를 보여주는 메서드
* classification_report 결과 예시 보기 

ROC Curve와 AUC
ROC(Receiver Operator Characteristic) Curve

-이진 분류기의 성능을 표현하는 커브

-가능한 모든 threshold(클래스 판별 기준값=임계값)에 대해 TPR(True Positive Rate)과 FPR(False Positive Rate)의 비율을 표현한 것

-ROC Curve가 좌상단에 붙을수록 분류기 성능이 좋다

TPR(=Recall=Sensitivity)
$$ TP/TP+FN $$

FPR(1-Specificity)
$$ FP/TN+FP $$

정밀도(Precision)/재현율(Recall) Trade-off
* threshold를 낮출 경우
분류기가 True라고 응답하는 경우 증가
➡️ TP와 FP가 높아지는 현상 발생. FN은 줄어드니(상대적으로 FP와 TP가 많아지므로) 
> 재현율(Recall)은 떨어지지만 정밀도(Precision)는 올라가게 된다

* threshold를 높일 경우
분류기는 False라고 응답하는 경우 증가 ➡️ TN과 FN이 높아지는 현상 발생(높은 신뢰 점수를 갖는 입력에만 True라고 응답하기 때문) FP는 줄어들어 재현율(Recall)은 올라가지만 정밀도(Precision)는 떨어진다

<aside> 💡 재현율(Recall) 최대화 & 위양성률(FPR) 최소화하는 적정한 임계값 찾아야 함

##1종오류/2종오류
* 1종 오류와 2종 오류는 기무가설, 대립가설에서 많이 쓰이는 용어
* 판단에 대한 오류를 의미
* 1종 오류 : 귀무가설이 실제로는 참이어서 채택해야 함에도 불구하고 오차때문에 이를 채택하지 않는 오류 ➡️ 참인데 거짓이라고 예측 → 헛다리? 짚는 것
* 2종 오류 : 귀무가설이 거짓이라서 채택하지 말아야 하는데 표본의 오차 때문에 이를 채택하는 오류 ➡️ 거짓인데 참이라고 예측 → 조금 답답한 사람
  
![스크린샷 2022-11-24 오후 5 48 27](https://user-images.githubusercontent.com/99530946/203735586-a8bf3735-52a0-416c-b5c1-010e79f65679.png)

	•	신약 개발 예시 가정 : 제약회사 연구자가 신약을 개발하고 있다. 신약의 효과를 검증하기 위한 임상시험을 진행하였고 그 바탕으로 신약이 효과가 정말 있는지 여부를 판단. 귀무가설 : 개발한 ‘신약은 효과가 없다’ 

1️⃣종오류 : 실제로 신약이 ‘효과가 없음’(참)에도 불구하고 ‘효과가 있다’고 판단하는 경우
2️⃣종오류 : 실제로 신약이 ‘효과가 있음’(거짓)에도 불구하고 ‘효과가 없다’고 판단하는 경우
⇒ 옳은 의사결정을 위해서는 1종 2종 오류 모두 최소화하는 연구 진행해야 함.

⁉️ 어떤 통계가 더 위험할까?
1종 오류 → 신약이 효과가 없는데 효과가 있다고 판단하는 경우
회사 뿐 아니라 그 약을 사용하는 환자들에게도 큰 피해
정밀도(Precision)와 재현율(Recall)
정밀도(Precision)
* 정밀도란 모델이 True라고 예측한 것 중에서 예측이 맞은 것의 비율
* 예측값이 1인 것을 기준으로 하는 계산

정밀도 중요한 경우
* 스팸 메일
스팸 메일이 맞으면 True, 스팸 메일이 아니면 False 라 할 때 스팸 메일이 아닌 경우인데 스팸 메일이라고 판단해서 스팸 메일을 차단하게 되면 중요한 메일을 받지 못하는 경우가 생긴다.
* 무고한 사람에게 유죄 선고
유죄가 맞으면 True, 유죄가 아니면 False 라 할 때 유죄가 아닌 사람에게 유죄라 판단해서 유죄 선고를 내리면 실제로 무고한 사람이 유죄 선고를 받는 경우가 생긴다.
재현율(Recall)
* 재현율이란 실제 True인 것 중에서 모델이 True로 나온 것의 비율
* 실제값이 1인것을 기준으로 하는 계산

재현율 중요한 경우
* 암 진단
암을 진단할 때 양성이면 True, 음성이면 False일 때 암이라고 예측한 환자중 실제로 암에 걸린 사람의 비율을 나타내는데 만약 실제로는 암 진단시 양성인데 음성이라고 판단하는 경우 위험하다.
* 금융 사기 적발
금융 사기를 적발할 때 금융 사기가 맞으면 True, 금융 사기가 아니라면 False 일 때 실제 금융 사기에 당한 것을 금융 사기가 아니라고 판단하는 경우 위험하다.


