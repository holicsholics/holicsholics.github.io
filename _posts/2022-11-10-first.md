---
layout: single
title:  "TIL_One_Hot_Encoding!"
---

11월 10일

https://wikidocs.net/book/2155 
위키독스, 딥러닝을 이용한 자연어처리 입문


One-Hot-Encoding
* 비정형 자료인 텍스트 자료를 정형화시키는 기본적인 아이디어는 문서를 이루고 있는 단어를 적당한 특징 벡터(feature vector)로 표현하는 것

* 벡터를 생성하는 가장 쉬운 방법은 해당 단어가 등장했으면 1, 등장하지 않았으면 0으로 표현하는 것

* 벡터의 크기는 단어 집합의 크기와 동일함 각 단어에 인덱스를 부여하여 특정 단어가 존재하면 벡터의 해당 인덱스를 1로, 나머지는 0으로 채움 (sparses representation)

* 예를 들어 [강아지, 고양이, 원숭이] 단어집합이 있는 경우 순서대로 인덱스를 1, 2, 3으로 부여한다고 가정
* 강아지: [1, 0, 0]

* 고양이: [0, 1, 0]
* 원숭이: [0, 0, 1]
자연어 처리에서의 One-Hot-Encoding

One-Hot-Encoding은 표현하고 싶은 단어에 1 다른 곳에는 0으로 표현함으로써 구현하는 벡터 표현 방식이다. 이 때 표현된 벡터는 (One-Hot-Vector)라고 한다. 위에 강아지와 고양이 원숭이도 모두 원-핫 벡터라고 볼 수 있다.

* 원-핫 인코딩의 과정

첫번째로 정수 인코딩을 수행한다. 이는 단어에 고유한 정수를 부여하는 작업이다. 
둘째로 표현 하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여한다. 다음 예시의 한국어 문장을 토대로 원-핫 벡터를 만들어 보자.

문장 : 나는 자연어 처리를 배운다

Okt 형태소 분석기를 통해서 문장에 대해서 토큰화를 수행합니다.
from konlpy.tag import Okt  

okt = Okt()  
tokens = okt.morphs("나는 자연어 처리를 배운다")  

print(tokens)

['나', '는', '자연어', '처리', '를', '배운다']

각 토큰에 대해서 고유한 정수를 부여한다. 지금은 문장이 짧기 때문에 각 단어의 빈도수를 고려하지 않지만, 빈도수 순으로 단어를 정렬하여 정수를 부여하는 경우가 많다.

word_to_index = {word : index for index, word in enumerate(tokens)}


print('단어 집합 :',word_to_index)

단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}

토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수를 만든다.

'''python
def one_hot_encoding(word, word_to_index):
  one_hot_vector = [0]*(len(word_to_index))
  index = word_to_index[word]
  one_hot_vector[index] = 1
  return one_hot_vector
'자연어'라는 단어의 원-핫 벡터를 얻어보자.
one_hot_encoding("자연어", word_to_index)
[0, 0, 1, 0, 0, 0]
'자연어'는 정수 2이므로 원-핫 벡터는 인덱스 2의 값이 1이며, 나머지 값은 0인 벡터가 나오게 된다!
'''

