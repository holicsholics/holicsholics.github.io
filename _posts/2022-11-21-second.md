---
layout: single
title:  "TIL_11월15일!"
---

## 배깅(Bagging)

**Bagging**은 Bootstrap Aggregation의 약자입니다. 배깅은 샘플을 여러 번 뽑아(Bootstrap) 각 모델을 학습시켜 결과물을 집계(Aggregration)하는 방법이다.

![Untitled](<https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ba8e063-c0eb-46c7-ba54-de6549dc12c9/Untitled.png>)

부트스트랩을 먼저 하는데 부트스트랩은 복원 랜덤 샘플링이다. 그 다음 부트스트랩한 데이터로 모델을 학습시키고 학습된 모델의 결과를 집계하여 최종 결과 값을 구하는 방식이다!

## 배깅을 활용한 예시

**랜덤포레스트(RandomForest)가 있다.**

### 랜덤포레스트란 :

하나의 결정트리가 모든 feature를 변수로 사용해서 y값을 예측한다면 앞서 말한대로 오버피팅 문제가 발생한다. 그래서 랜덤포레스트는 feature를 무작위로 뽑거나, 데이터를 무작위로 뽑아서 여러개의 작은 트리를 만들고 그 트리들을 결합한다.

예를 들어 feature가 30개가 있다면 그 중에 랜덤하게 5개만 뽑아서 트리를 하나 만들고, 또 다시 랜덤하게 5개의 feature를 뽑아서 두번째 트리를 만들고, 이런 식으로 트리를 여러 개 만드는 것이 바로 랜덤포레스트이다.

트리를 여러 개 만들면 트리 개수만큼 예측 결과값이 생성되는데 voting을 통해 결과값을 채택하게 된다. 이게 바로 앙상블 학습이다.

랜덤포레스트는 기본적으로 결정트리의 단점을 보완하는 모델이라 오버피팅이 적고 성능이 뛰어나다. n_estimators, max_features, max_depth 정도와 같은 몇몇 파라미터만 잘 튜닝해주면 튜닝을 많이 하지 않아도 높은 성능을 보인다.

### **랜덤포레스트의 주요 파라미터:**

**n_estimators**: 트리를 몇 개 만들 것인지 (int, default=100), 값이 클수록 오버피팅 방지

**criterion**: gini 또는 entropy 중 선택

**max_depth**: 트리의 깊이 (int, default=None)

**bootstrap**: True이면 전체 feature에서 복원추출해서 트리 생성 (default=True)

**max_features**: 선택할 feature의 개수, 보통 default값으로 씀 (default='auto')

### 랜덤포레스트 예제
